{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ShakespearGPT\n",
        "### Dans ce notebook, je vais essayer de reproduire le ShakespearGPT proposé par Andrej Karpathy dans sa vidéo YouTube. Je fais cela dans un but éducatif, pour comprendre le fonctionnement des LLMs.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ecq-EVYZHT3u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## I - Importation et préparation des données"
      ],
      "metadata": {
        "id": "Bq1F6n2FIUnW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "C'est Andrej Karpathy qui le dit, on commence toujours par importer les données. Ici, on importe le contenu de TinyShakespear, qui est une concaténation de l'ensemble des écrits de Shakespeare.\n",
        "\n",
        "J'importe son contenu dans ce notebook dans le fichier input.txt que je récupère sur le repo github du projet.\n",
        "\n",
        "Je l'importe ci-dessous dans la variable string text."
      ],
      "metadata": {
        "id": "yd0i3iVFIdWG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "icYzZ3E8HMKW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80f9d0e6-f5b9-426f-80e6-641b075d3484"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-20 07:00:58--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-10-20 07:00:58 (22.3 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "input_file = open('input.txt', 'r', encoding='utf-8')\n",
        "text = input_file.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maintenant qu'on a nos données, comme tout bon ingénieur, on va s'intéresser un peu à ce qu'on va manipuler avant d'aller plus loin."
      ],
      "metadata": {
        "id": "Soe-zZeaMdsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Le texte contient\", len(text), \"caractères.\")\n",
        "print(\"Le texte contient\", len(set(text)), \"caractères uniques.\")\n",
        "print(\"\\nLes 500 premiers caractères du texte sont :\\n\", text[:500])\n",
        "print(\"\\nLes caractères de 24314 à 24414 du texte sont :\\n\", text[24314:24414])"
      ],
      "metadata": {
        "id": "9kJq8m40M_p2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f77ec3df-1428-4e72-ff1c-01513ffea80a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le texte contient 1115394 caractères.\n",
            "Le texte contient 65 caractères uniques.\n",
            "\n",
            "Les 500 premiers caractères du texte sont :\n",
            " First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor\n",
            "\n",
            "Les caractères de 24314 à 24414 du texte sont :\n",
            " lish in our stands,\n",
            "Nor cowardly in retire: believe me, sirs,\n",
            "We shall be charged again. Whiles we h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, on a un peu plus d'informations sur le texte, et surtout on sait qu'on n'a pas importé n'importe quoi : ça ressemble bien à du shakespear.\n",
        "Une information importante qu'on a maintenant, c'est le nombre de caractère différents dans le texte : 65.\n",
        "Notre GPT devra choisir, à chaque itération, un caractère parmi 65.\n",
        "\n",
        "Étudions un peu ces caractères."
      ],
      "metadata": {
        "id": "T1rtv-vAN0Aa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(chars)"
      ],
      "metadata": {
        "id": "UGDlkTodO7g5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e3bbc7b-e33e-411a-9d7e-174a85c8a995"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Toutes les lettres en majuscule, toutes les lettres en minuscule, espace, saut à la ligne, dollars (??), point etc.\n",
        "\n",
        "Maintenant qu'on a bien étudié nos données, qu'on s'est fait une plutôt bonne idée de ce qui les constituait, on va passer à la première étape importante : l'encodage et le décodage en tokens.\n",
        "\n",
        "Un token, c'est un morceau de texte que notre GPT peut prédire à chaque itération. Pour faire simple, c'est une unité de texte. Pourquoi est-ce qu'on a besoin des tokens ? Ça permet de transformer le problème de génération de texte en problème de génération d'entier. En plus, on peut choisir nos tokens de différentes manières, certaines plus efficaces que d'autres.\n",
        "\n",
        "Dans notre cas, on va pas se prendre trop la tête, on a un plutôt bon candidat : les caractères. Chaque caractère sera un token. Pour les encoder, il suffit donc d'associer à chaque caractère un entier.\n",
        "\n",
        "Programmons l'encodage et le décodage, pour obtenir les tokens qu'on utilisera pour ce projet."
      ],
      "metadata": {
        "id": "_XhFsksGPrn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "str_to_int = { ch:i for i,ch in enumerate(chars) }\n",
        "int_to_str = { i:ch for i,ch in enumerate(chars) }\n",
        "\n",
        "# L'encodage ci-dessous suppose que l'entrée ne contient que les caractères de notre texte\n",
        "def encode(s) :\n",
        "  res = []\n",
        "  for i in range(len(s)) :\n",
        "    res.append(str_to_int[s[i]])\n",
        "  return res\n",
        "\n",
        "def decode(s) :\n",
        "  res = \"\"\n",
        "  for i in range(len(s)) :\n",
        "    res += chars[s[i]]\n",
        "  return res\n",
        "\n",
        "print(encode(\"Ceci est un texte que je vais encoder puis decoder.\"))\n",
        "print(decode(encode(\"Ceci est un texte que je vais encoder puis decoder.\")))"
      ],
      "metadata": {
        "id": "xJAF2tXNSYpw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2afe86a4-a049-441f-d29a-8efa8c1612a3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15, 43, 41, 47, 1, 43, 57, 58, 1, 59, 52, 1, 58, 43, 62, 58, 43, 1, 55, 59, 43, 1, 48, 43, 1, 60, 39, 47, 57, 1, 43, 52, 41, 53, 42, 43, 56, 1, 54, 59, 47, 57, 1, 42, 43, 41, 53, 42, 43, 56, 8]\n",
            "Ceci est un texte que je vais encoder puis decoder.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sur cet exemple, on comprend un peu pourquoi notre encodage n'est pas optimal. \"est\" par exemple est une séquence qui apparaîtra très fréquemment dans un texte en français, ça serait donc pratique d'avoir un token dédier. En choisissant bien les tokens, on peut réduire la taille de l'encodage. Il faut en réalité trouver le bon équilibre entre nombre de tokens et taille des encodages. L'encodage de GPT2, par exemple, comptait plus de 50000 tokens.\n",
        "\n",
        "Ceci dit, notre encodage, assez simple à comprendre, fera l'affaire pour ce projet.\n",
        "\n",
        "Maintenant qu'on a défini tokens et encodage, appliquons tout ça à notre texte."
      ],
      "metadata": {
        "id": "EZ6SlQB6r4k-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHfMOibgsdwC",
        "outputId": "39edc6a4-1eda-4da3-bfb9-2201c55705db"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En analysant un peu ce qui apparaît, on remarque d'abord que on a bien autant d'entier dans data que de caractères dans text, puis que notre encodage a l'air d'avoir bien fonctionné, si l'on compare les premiers entiers aux premiers caractères.\n",
        "\n",
        "On a nos données, prêtes à entraîner notre modèle... À un détail prêt : pour s'assurer que le modèle fonctionne toujours bien sur des nouvelles données, qu'il n'est pas seulement bon à reproduire ce qu'on lui a donné pour s'entraîner, on va séparer nos données en deux ensemble : le training set et le validation set"
      ],
      "metadata": {
        "id": "NEoGgZlVvqDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "sxCTgCX4wsAM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voilà, nos données sont séparées en un ensemble d'entraînement, et un ensemble de validation.\n",
        "\n",
        "Maintenant, on ne va pas donner un texte entier et demander de prédire le caractère suivant. On va donner à notre algorithme des blocks de texte encodé, et lui demander de prédire, selon ce petit contexte, le caractère suivant.\n",
        "\n",
        "Ici, on va considérer des blocks de 8 tokens."
      ],
      "metadata": {
        "id": "RSU2CcHoxgI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "\n",
        "print(train_data[:block_size+1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_9sWZPdyn8M",
        "outputId": "abb913fd-529d-44f2-b7fa-76dbbb7fbd61"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ça, c'est un block d'entrée et le caractère qui suit.\n",
        "Mais si on fait l'effort de regarder entre les caractères, on s'aperçoit que ce block de taille block_size en contient en réalité 8.\n",
        "\n",
        "En effet,on va entraîner notre modèle sur des entrées de taille block_size, mais on veut aussi qu'il soit capable de faire une prédiction en se basant uniquement sur un contexte d'un seul caractère.\n",
        "\n",
        "Ainsi, un block de 8 tokens nous donne en réalité 8 prédictions à faire pour s'entraîner."
      ],
      "metadata": {
        "id": "r6rT6xFE5x01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size) :\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f\"Lorsque le contexte est {context}, la cible est {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UswqqyHP57X0",
        "outputId": "f49eb918-e609-4893-a62c-9afca55162dc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lorsque le contexte est tensor([18]), la cible est 47\n",
            "Lorsque le contexte est tensor([18, 47]), la cible est 56\n",
            "Lorsque le contexte est tensor([18, 47, 56]), la cible est 57\n",
            "Lorsque le contexte est tensor([18, 47, 56, 57]), la cible est 58\n",
            "Lorsque le contexte est tensor([18, 47, 56, 57, 58]), la cible est 1\n",
            "Lorsque le contexte est tensor([18, 47, 56, 57, 58,  1]), la cible est 15\n",
            "Lorsque le contexte est tensor([18, 47, 56, 57, 58,  1, 15]), la cible est 47\n",
            "Lorsque le contexte est tensor([18, 47, 56, 57, 58,  1, 15, 47]), la cible est 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On commence à se faire une bonne idée de ce qu'on va donner en entrée à notre modèle. La dernière chose à faire, c'est de rassembler plusieurs entrées en **batch**.\n",
        "\n",
        "Entraîner sur une donnée, ça prend un certain temps. Or, pendant qu'on entraîne sur une donnée, il nous reste de la place pour en entraîner d'autres en parallèle.\n",
        "\n",
        "On ne va donc pas donner nos entrées une par une, mais les rassembler en **batch**, qui désigne un ensemble d'entrées mises en parallèle.\n",
        "\n",
        "Il reste donc à définir ces batchs, après quoi nous auront nos données prêtes pour entraîner notre modèle."
      ],
      "metadata": {
        "id": "aLEd-3g_7auU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4\n",
        "\n",
        "def get_batch(training) :\n",
        "  data = train_data if training else val_data\n",
        "  random_sample_index = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in random_sample_index]) #torch.stack concatenates tensors given in one tensor of higher dimension\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in random_sample_index])\n",
        "  return x, y\n",
        "\n",
        "xb, yb = get_batch(training = True)\n",
        "print(\"inputs :\")\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print(\"targets :\")\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "for b in range(batch_size) :\n",
        "  for t in range(block_size) :\n",
        "    context = xb[b, :t+1]\n",
        "    target = yb[b, t]\n",
        "    print(\"Quand l'entrée est\", context, \"la prédiction attendue est :\", target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IEiH1JM8j1u",
        "outputId": "e2c84204-849e-4209-cac9-498cf2f19d1b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs :\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets :\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "\n",
            "\n",
            "\n",
            "Quand l'entrée est tensor([24]) la prédiction attendue est : tensor(43)\n",
            "Quand l'entrée est tensor([24, 43]) la prédiction attendue est : tensor(58)\n",
            "Quand l'entrée est tensor([24, 43, 58]) la prédiction attendue est : tensor(5)\n",
            "Quand l'entrée est tensor([24, 43, 58,  5]) la prédiction attendue est : tensor(57)\n",
            "Quand l'entrée est tensor([24, 43, 58,  5, 57]) la prédiction attendue est : tensor(1)\n",
            "Quand l'entrée est tensor([24, 43, 58,  5, 57,  1]) la prédiction attendue est : tensor(46)\n",
            "Quand l'entrée est tensor([24, 43, 58,  5, 57,  1, 46]) la prédiction attendue est : tensor(43)\n",
            "Quand l'entrée est tensor([24, 43, 58,  5, 57,  1, 46, 43]) la prédiction attendue est : tensor(39)\n",
            "Quand l'entrée est tensor([44]) la prédiction attendue est : tensor(53)\n",
            "Quand l'entrée est tensor([44, 53]) la prédiction attendue est : tensor(56)\n",
            "Quand l'entrée est tensor([44, 53, 56]) la prédiction attendue est : tensor(1)\n",
            "Quand l'entrée est tensor([44, 53, 56,  1]) la prédiction attendue est : tensor(58)\n",
            "Quand l'entrée est tensor([44, 53, 56,  1, 58]) la prédiction attendue est : tensor(46)\n",
            "Quand l'entrée est tensor([44, 53, 56,  1, 58, 46]) la prédiction attendue est : tensor(39)\n",
            "Quand l'entrée est tensor([44, 53, 56,  1, 58, 46, 39]) la prédiction attendue est : tensor(58)\n",
            "Quand l'entrée est tensor([44, 53, 56,  1, 58, 46, 39, 58]) la prédiction attendue est : tensor(1)\n",
            "Quand l'entrée est tensor([52]) la prédiction attendue est : tensor(58)\n",
            "Quand l'entrée est tensor([52, 58]) la prédiction attendue est : tensor(1)\n",
            "Quand l'entrée est tensor([52, 58,  1]) la prédiction attendue est : tensor(58)\n",
            "Quand l'entrée est tensor([52, 58,  1, 58]) la prédiction attendue est : tensor(46)\n",
            "Quand l'entrée est tensor([52, 58,  1, 58, 46]) la prédiction attendue est : tensor(39)\n",
            "Quand l'entrée est tensor([52, 58,  1, 58, 46, 39]) la prédiction attendue est : tensor(58)\n",
            "Quand l'entrée est tensor([52, 58,  1, 58, 46, 39, 58]) la prédiction attendue est : tensor(1)\n",
            "Quand l'entrée est tensor([52, 58,  1, 58, 46, 39, 58,  1]) la prédiction attendue est : tensor(46)\n",
            "Quand l'entrée est tensor([25]) la prédiction attendue est : tensor(17)\n",
            "Quand l'entrée est tensor([25, 17]) la prédiction attendue est : tensor(27)\n",
            "Quand l'entrée est tensor([25, 17, 27]) la prédiction attendue est : tensor(10)\n",
            "Quand l'entrée est tensor([25, 17, 27, 10]) la prédiction attendue est : tensor(0)\n",
            "Quand l'entrée est tensor([25, 17, 27, 10,  0]) la prédiction attendue est : tensor(21)\n",
            "Quand l'entrée est tensor([25, 17, 27, 10,  0, 21]) la prédiction attendue est : tensor(1)\n",
            "Quand l'entrée est tensor([25, 17, 27, 10,  0, 21,  1]) la prédiction attendue est : tensor(54)\n",
            "Quand l'entrée est tensor([25, 17, 27, 10,  0, 21,  1, 54]) la prédiction attendue est : tensor(39)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## II - Construction d'un premier modèle BLM"
      ],
      "metadata": {
        "id": "Jk6IF2qM2m7F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nos données sont prêtes, au bon format pour notre transformer!\n",
        "\n",
        "On peut maintenant s'intéresser au modèle que l'on va implémenter.\n",
        "\n",
        "Pour commencer cela, on va partir de la base des NLP : les Bigram Language Model. Un BLM, c'est un modèle dont l'unique rôle est de prendre un token, et de prédire celui qui suit. Il y a plusieurs manières de le faire. On peut prendre l'ensemble des données, regarder la fréquence d'apparition de chaque couple de token possible, en déduire la prédiction la plus probable pour le token suivant. On peut aussi construire un réseau neuronal qui va se charger de faire la prédiction, et c'est ce qu'on va faire ici.\n",
        "\n",
        "1) Forward\n",
        "\n",
        "Étant donné un batch xb en entrée et les targets yb correspondantes, notre modele va construire une **embedding table de dimension batch_size * block_size * num_classes** selon xb, qui donnera, pour chaque entrée du batch, les scores (~probas) de chaque token pour toutes les sous-entrées. Un peu compliqué tout ça.\n",
        "Avec cette embedding table, le modèle fera sa prédiction, et on calcule ensuite a perte avec cross_entropy. Pour faire cela, on ajuste les dimensions de notre prédiction et les targets (pour coller aux entrées attendues par la fonction torch.functional.cross_entropy).\n",
        "\n",
        "2) Generate\n",
        "\n",
        "On passe à l'étape plus fun, le résultat voulu : la génération de texte. Pour cette fonction, on répète juste autant de fois qu'on veut la prédiction du prochain caractère basé uniquement sur le caractère précédent. On garde la prédiction qui a la probabilité la plus élevée, on l'ajoute à l'entrée, et on répète le procédé sur notre entrée agrandie."
      ],
      "metadata": {
        "id": "asxuDgcjfOTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "vocab_size = len(chars)\n",
        "n_embd = 32\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class FirstBiagramLanguageModel(nn.Module) :\n",
        "  def __init__(self) :\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # matrice de taille vocab_size * n_embd\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd) # matrice de taille block_size * n_embd\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size) # matrice de taille n_embd * vocab_size, qui permettra de transformer token_embd en logits\n",
        "\n",
        "  def forward(self, idx, targets=None) :\n",
        "    B, T = idx.shape\n",
        "\n",
        "    tok_emb = self.token_embedding_table(idx) # idx est de dimensions B * T, tok_emb est donc de dimensions B * T * C (où C = n_embd)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device = device)) # dimensions T * C, ajoute l'information de la position du token dans le block\n",
        "    x = tok_emb + pos_emb # dimensions B * T * C\n",
        "    logits = self.lm_head(x) # idx est de dimensions B * T, logits est donc de dimensions B * T * vocab_size\n",
        "\n",
        "    if targets == None :\n",
        "      loss = None\n",
        "    else :\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C) # on redimensionne nos prédictions pour calculer la loss\n",
        "      targets = targets.view(B*T)\n",
        "\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_tokens_generated) :\n",
        "    for i in range(max_tokens_generated) :\n",
        "      idx_cond = idx[:, -block_size:]  # tronquer à la longueur maximale autorisée\n",
        "      logits, loss = self.forward(idx_cond)\n",
        "      logits = logits[:, -1, :] # on isole les prédictions pour le prochain caractère basé sur celui qui précède\n",
        "      probs = F.softmax(logits, dim=-1) # on calcule les probas\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # on sélectionne le caractère suivant selon les probas obtenues\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # on ajoute le caractère suivant à notre entrée\n",
        "    return idx\n",
        "\n",
        "m = FirstBiagramLanguageModel()\n",
        "logits, loss = m(xb, yb)\n",
        "\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "idx = torch.zeros((1, 1), dtype=torch.long) #on donne en premier le caractère de saut de ligne\n",
        "print(decode(m.generate(idx, max_tokens_generated=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfP2B_BDf9EZ",
        "outputId": "bce3451d-3dee-4545-859e-10a13d4f054c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.6424, grad_fn=<NllLossBackward0>)\n",
            "\n",
            ":RTbVTkMTUwF C$?3fHvOvsmEEDDoys!SZgyGrRX:DdqBsmroU&SjrPr:EjT!hjmfHDHd3cOx.vvgvuvL&egm-CvLif.z Ur3RmC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On a un modèle bien construit, et une belle fonction de génération qui est déjà construite de sorte à pouvoir, plus tard, prendre en compte le contexte, et pas seulement le dernier caractère !\n",
        "\n",
        "Par contre, le résultat n'est pas fou.. En même temps, on le génère complétement aléatoirement. On va entraîner notre modèle pour voir s'il peut s'améliorer.\n",
        "\n",
        "On va utiliser le AdamW optimizer de pytorch, et lancer une boucle d'optimisation plutôt classique : on prend prend un batch aléatoire, on fait une prédiction, on calcule la loss, on calcule les gradients, puis on optimise en conséquence\n",
        "\n",
        "On va voir jusqu'ou on peut descendre en loss avec cette optimisation."
      ],
      "metadata": {
        "id": "fGmmAKpV_yWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
        "\n",
        "batch_size = 32\n",
        "iterations = 10000\n",
        "\n",
        "for i in range(iterations) :\n",
        "  xb, yb = get_batch(training = True)\n",
        "  logits, loss = m(xb, yb)\n",
        "\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCI75qnoBcjD",
        "outputId": "a644e361-03b4-4a72-9f17-7f1671d76e26"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.384690761566162\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En répétant 1000 itérations, on descend à une perte de 3.7, en répétant l'optimisation encore 10000 fois on descend jusqu'à 2.56, puis encore 10000 itérations pour descendre à une perte de 2.41.\n",
        "\n",
        "Ok on voit que la perte a bien descendu, donc on va réessayer de générer du texte. Ça ne sera probablement pas du Shakespear, mais sûrement un peu plus compréhensible que ce qu'on avait généré sans entraînement."
      ],
      "metadata": {
        "id": "JxCKtgHwCKdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((1, 1), dtype=torch.long) #on donne en premier le caractère de saut de ligne\n",
        "print(decode(m.generate(idx, max_tokens_generated=300)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xe1jVgGuDE_C",
        "outputId": "c2d67037-81ea-49d7-f891-c5126c2dd0d3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "ARKI it?\n",
            "Angirounesoacat the Fat haceseliss me--s t ese we ds\n",
            "\n",
            "NCHULI:\n",
            "OMet monce : Stus me h! al-p blllkscet pe lar ce, haw, d our, ly be d p, beleer\n",
            "ANCande my innofoothesound is'lf co o he sottcothealin athe ot bande s wnetourmapuseas ated a t.\n",
            "O:\n",
            "\n",
            "T:\n",
            "GLLINGLARotind ile hingoue arilo se\n",
            "AUSewo g\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On ne va pas attirer beaucoup de monde dans les théâtres avec un texte comme ça, mais on commence à reconnaître la forme d'une pièce de théâtre.\n",
        "\n",
        "Le problème, c'est qu'on ne fait nos prédictions que sur le dernier caractère. Pour obtenir un résultat satisfaisant, il va falloir commencer à prendre en compte le contexte.\n",
        "\n",
        "C'est là que ça devient intéressant."
      ],
      "metadata": {
        "id": "KUgMhFZgDuFO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## III - Introduction mathématique à la self-attention"
      ],
      "metadata": {
        "id": "0oEx1xQEHIBI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Avant d'avancer, on va essayer de bien comprendre ce qui se passe derrière le principe de self-attention. Comprendre l'idée mathématique est la clé d'une bonne implémentation. On va faire un exemple pour illustrer tout ça."
      ],
      "metadata": {
        "id": "eLp7LyJ4HRpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B, T, C = 4, 8, 2\n",
        "x = torch.randn(B, T, C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fH0UpQG2HPBz",
        "outputId": "299ae1e3-590d-403b-cec2-417ebbe55f27"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On a construit une entrée aléatoire (ce n'est pas l'entrée qui importe, mais ce qu'on va en faire).\n",
        "\n",
        "L'idée est la suivante, pour le caractère à l'instant t, on veut conserver une information des caractères à l'instant t-1, t-2 etc. Ainsi, à tout instant, le caractère conserve une empreinte de ceux qui le précèdent.\n",
        "\n",
        "Comment conserver l'information ? Il y a plein de manière de le faire, mais on va faire au plus simple pour l'instant : on va faire la moyenne des caractères qui précèdent et du caractère actuel pour obtenir notre nouveau caractère."
      ],
      "metadata": {
        "id": "wMePtgE8ItgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xbow = torch.zeros((B,T,C)) # bow stands for bag of words -> on fait juste la moyenne d'un groupe de mots (ici des caractères)\n",
        "for b in range(B) :\n",
        "  for t in range(T) :\n",
        "    xprev = x[b, :t+1]\n",
        "    xbow[b, t] = torch.mean(xprev, 0)"
      ],
      "metadata": {
        "id": "_Nac0py0J-gA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le code ci-dessus nous donne xbow qui est le résultat de l'opération qu'on voulait faire. Seulement, on fait notre calcul d'une manière pas très efficace.\n",
        "\n",
        "On va voir comment rendre ce calcul efficace par le calcul marticiel."
      ],
      "metadata": {
        "id": "-vr8ybEsORhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0, 10, (3, 2)).float()\n",
        "c = a @ b\n",
        "\n",
        "print(\"tril :\")\n",
        "print(a)\n",
        "print(\"\\n\")\n",
        "print(\"b :\")\n",
        "print(b)\n",
        "print(\"\\n\")\n",
        "print(\"c :\")\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8wq1r4aVGNI",
        "outputId": "0256b795-33ec-42ff-9703-d041d3a81dfb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tril :\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "\n",
            "\n",
            "b :\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "\n",
            "\n",
            "c :\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Avec ce calcul matriciel, et plus précisément, avec cette matrice a, on peut faire le calcul de xbow (ici c) étant donné x (ici b) de manière efficace."
      ],
      "metadata": {
        "id": "Y1kQVHbPVpZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # Les dimensions (T, T) et (B, T, C) ne permettent pas une multiplication matricielle,\n",
        "                # l'opérateur @ crée une dimension B, donc l'opération est appliquée à toutes les entrée du batch,\n",
        "                # le tout en parallèle donc plus efficacement"
      ],
      "metadata": {
        "id": "cA9e-LIyVqLO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On peut également réaliser cette opération, obtenir la même matrice wei avec un softmax (en l'appliquant à la matrice tril ou les 1 deviennent 0 et les 0 deviennent -inf). C'est cette méthode là que l'on va retenir pour la suite."
      ],
      "metadata": {
        "id": "NctD0QyolsKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T, T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x"
      ],
      "metadata": {
        "id": "i1pSa7SgnVAe"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce qu'il faut retenir de tout ça, c'est qu'on peut réécrire une entrée pour que chaque caractère représente aussi le contexte dans lequel il apparaît, à l'aide d'une simple multiplication matricielle. La matrice wei définit les poids, et donc l'importance, l'affinité, de chaque caractère passés."
      ],
      "metadata": {
        "id": "tYL1K8Vkocko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IV - Self-Attention"
      ],
      "metadata": {
        "id": "EQCDFG8QzewN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On arrive enfin au principe clé de notre algorithme : la self-attention.\n",
        "\n",
        "On a vu juste avant qu'on pouvait garder les informations des caractères passés, mais on l'a fait pour l'instant d'une manière assez simple, juste avec la moyenne des caractères précédents. On aimerait bien donner plus d'importance aux caractères qui portent plus d'information. Pour ça, on va rendre notre matrice de poids dépendante des données. On va, elle aussi, l'entraîner, pour retenir l'essentiel de l'information du contexte.\n",
        "\n",
        "Pour ce faire, chaque token émettra deux vecteurs : un vecteur key, et un vecteur query. Le vecteur key décrira les informations qui constituent le token, le vecteur query décrira les informations dont le token a besoin. Avec de l'entraînement, le produit du vecteur query d'un caractère avec celui key d'un caractère avec aura une valeur élevée si leur affinité est importante.\n",
        "\n",
        "Pour implémenter le principe de self-attention, on met en place des heads.\n",
        "\n"
      ],
      "metadata": {
        "id": "HZfmbT4ozj2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B, T, C = 4, 8, 32\n",
        "x = torch.randn(B, T, C)\n",
        "\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x) # (B, T, 16)\n",
        "q = query(x) #(B, T, 16)\n",
        "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "# wei donne, pour chaque entrée du batch, une pondération de l'information des caractères passés\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1) # Le softmax permet de normaliser notre pondération, en donnant de l'importance aux key(token_i).query(current_token)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQWWlabkCSqb",
        "outputId": "09f0c2a8-714a-48cc-b8d3-e0bfef5a133c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le principe d'attention est un méchanisme de communication qui peut être représenté par un graphe. Les différents sommets du graphe envoie des informations via leurs arcs. Dans notre cas, on a huit sommets alignés, (numérotés pour garder la notion d'espace qu'il n'y a pas dans l'attention) le premier est relié à tous les autres, le deuxième à tous sauf au premier etc (ils donnent l'information aux caractères suivants).\n",
        "\n",
        "Le principe d'attention, c'est juste une manière d'indiquer qu'on définit un système de communication de l'information dans nos données d'entrée.\n",
        "\n",
        "La self-attention, ça signifie simplement que les keys et les queries ont les mêmes sources (ici les tokens d'une entrée).\n",
        "\n",
        "On va maintenant créer la classe Head que l'on utilisera pour mettre tout ça en ordre."
      ],
      "metadata": {
        "id": "C9Uvy9_IVA1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module) :\n",
        "  def __init__(self, head_size) :\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "  def forward(self, x) :\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "    # Calcul des affinités entre les caractères\n",
        "    wei = q @ k.transpose(-2, -1)\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "    wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "    v = self.value(x)\n",
        "    out = wei @ v\n",
        "    return out\n",
        "\n"
      ],
      "metadata": {
        "id": "xyvtO090-rbL"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maintenant qu'on a notre classe Head, on va aller encore un peu plus loin.\n",
        "On va s'intéresser aux MultiHeadAttention.\n",
        "\n",
        "Ce sont, en gors, simplement des ensemble de Heads, et plutôt que de simplement appliquer une Head à une entrée x, on lui applique plein de heads."
      ],
      "metadata": {
        "id": "vxyZGTltXw6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module) :\n",
        "  def __init__(self, num_heads, head_size) :\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "\n",
        "  def forward(self, x) :\n",
        "    return torch.cat([h(x) for h in self.heads], dim=-1)"
      ],
      "metadata": {
        "id": "SyL9g0mOefOo"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On va ensuite avoir besoin d'un FeedForward, qui est simplement une couche linéaire de neurones suivis d'une activation non linéaire (ReLU).\n",
        "\n",
        "Pourquoi a-t-on besoin de ce Feed Forward ?\n",
        "\n",
        "Avec notre système Multi Heads pour implémenter la self attention, les tokens ont communiqué entre eux. Une fois qu'on l'a appliqué à l'entrée, les tokens ne sont plus seulement de simples tokens, ils portent l'information du contexte dans lequel ils apparaissent.\n",
        "\n",
        "Maintenant, ce qu'il reste à faire semble évident : on a des informations bien optimisées, maintenant il va falloir choisir en se basant sur nos informations. C'est le rôle du Feed Forward.\n",
        "\n",
        "Pour résumer :\n",
        "1) On a nos entrées et leur positionnement.\n",
        "2) On leur applique une self attention avec des multi heads pour optimiser le partage d'informations\n",
        "3) On fait notre prédiction avec le feed forward"
      ],
      "metadata": {
        "id": "RnaIwEmTZEWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module) :\n",
        "  def __init__(self, n_embd) :\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd, 4 * n_embd),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4 * n_embd, n_embd)\n",
        "    )\n",
        "\n",
        "  def forward(self, x) :\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "vD9KX9YBZSie"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maintenant qu'on a défini nos simple Heads, nos Multi Heads, on peut s'en servir dans la définition de notre modèle."
      ],
      "metadata": {
        "id": "MD9A355rWVhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self) :\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # matrice de taille vocab_size * n_embd\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd) # matrice de taille block_size * n_embd\n",
        "    self.sa_heads = MultiHeadAttention(4, n_embd//4)\n",
        "    self.ffwd = FeedForward(n_embd)\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size) # matrice de taille n_embd * vocab_size, qui permettra de transformer token_embd en logits\n",
        "\n",
        "  def forward(self, idx, targets=None) :\n",
        "    B, T = idx.shape\n",
        "\n",
        "    tok_emb = self.token_embedding_table(idx) # idx est de dimensions B * T, tok_emb est donc de dimensions B * T * C (où C = n_embd)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device = device)) # dimensions T * C, ajoute l'information de la position du token dans le block\n",
        "    x = tok_emb + pos_emb # dimensions B * T * C\n",
        "    x = self.sa_heads(x) # On applique une tête de la self attention\n",
        "    x = self.ffwd(x)\n",
        "    logits = self.lm_head(x) # idx est de dimensions B * T, logits est donc de dimensions B * T * vocab_size\n",
        "\n",
        "    if targets == None :\n",
        "      loss = None\n",
        "    else :\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C) # on redimensionne nos prédictions pour calculer la loss\n",
        "      targets = targets.view(B*T)\n",
        "\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_tokens_generated) :\n",
        "    for i in range(max_tokens_generated) :\n",
        "      idx_cond = idx[:, -block_size:]  # tronquer à la longueur maximale autorisée\n",
        "      logits, loss = self.forward(idx_cond)\n",
        "      logits = logits[:, -1, :] # on isole les prédictions pour le prochain caractère basé sur celui qui précède\n",
        "      probs = F.softmax(logits, dim=-1) # on calcule les probas\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # on sélectionne le caractère suivant selon les probas obtenues\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # on ajoute le caractère suivant à notre entrée\n",
        "    return idx\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "# ------------\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_tokens_generated=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mG3s2mgOX6yW",
        "outputId": "d8b40260-dfb6-459c-dab6-2b6789b2d3f3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.1960, val loss 4.1948\n",
            "step 300: train loss 2.3696, val loss 2.3766\n",
            "step 600: train loss 2.2780, val loss 2.2883\n",
            "step 900: train loss 2.2368, val loss 2.2521\n",
            "step 1200: train loss 2.1875, val loss 2.1879\n",
            "step 1500: train loss 2.1620, val loss 2.1630\n",
            "step 1800: train loss 2.1441, val loss 2.1573\n",
            "step 2100: train loss 2.1308, val loss 2.1383\n",
            "step 2400: train loss 2.1154, val loss 2.1404\n",
            "step 2700: train loss 2.1025, val loss 2.1138\n",
            "\n",
            "IUC't I:\n",
            "Sew civy? ou shok y lon nin b me dous, ois ton mal by orelot KETh hin ay vield sur in:\n",
            "O:\n",
            "CES:\n",
            "\n",
            "asckist;\n",
            "TELoon cibef thevepug tout mureasad,\n",
            "NARDoredroon buperng rsergad,\n",
            "Males, eithor d.\n",
            "MCThere,\n",
            "O bat ig, r:\n",
            "I ath, thithineld fuefacul:\n",
            "Whestre hind\n",
            "F bemeelem.\n",
            "RD:\n",
            "Paill,\n",
            "LEdud ak toof thoth y?\n",
            "k,\n",
            "CESCEs t ceishiolof bad inta o'thaind ne lenslare.\n",
            "NGkenou st tistllay s.\n",
            "BAfowhe h ld y fe in w tinolousish.\n",
            "ENCARCHity BRTISLOBy'shuks n ingobetarde thtofooonoull pavishul uland ty s; he t\n"
          ]
        }
      ]
    }
  ]
}